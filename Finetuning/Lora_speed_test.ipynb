{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b618e2-143c-4645-89c1-5f9d2b66482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaForCausalLM,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "import time\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db51b8fe-a66b-4f04-a656-490ec0b5e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c842ff70-f8c6-4eb8-b61b-a3c9be21d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True,  \n",
    "                                bnb_4bit_quant_type=\"nf4\",\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5a8164-4cb3-4918-93a9-4aab5f3737ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= [\"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84818aed-dd55-4551-a236-ac757ecdb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, runs=10):\n",
    "    inp = tokenizer.encode(\"This is a long story about Alise in wonderland.\\n\", return_tensors=\"pt\").to(\"cuda\")\n",
    "    len_inp = len(inp[0])\n",
    "    generated_tokens = 0\n",
    "    with torch.inference_mode():\n",
    "        # load model\n",
    "        _ = model.generate(inp, max_new_tokens=1000, do_sample=False, num_beams=1, temperature=0, top_p=1.0)\n",
    "        t_start = time.perf_counter()\n",
    "        for _ in range(runs):\n",
    "            out = model.generate(inp, max_new_tokens=1000, do_sample=False, num_beams=1, temperature=0, top_p=1.0)\n",
    "            generated_tokens += len(out[0]) - len_inp\n",
    "        t_total = time.perf_counter() - t_start\n",
    "    del(inp)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return {\"time_total\": t_total, \"generated_tokens\": generated_tokens, \"tokens/sec\": generated_tokens / t_total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94638da-ae29-4810-802b-0864256f5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, use_fast=True)\n",
    "#Create a new token and add it to the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0af6ea-3bf6-4d7f-9edf-4322372efefa",
   "metadata": {},
   "source": [
    "---\n",
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad35f61c-9a6a-45c8-a3b9-33c4c786fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22d93df6-90bc-4237-8dba-62a620463faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=None,\n",
    "            attn_implementation=\"sdpa\",\n",
    "            device_map=(\"cuda\"),\n",
    "            torch_dtype=torch.bfloat16)\n",
    "model_original = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ddc3297-c6fb-4d7b-88aa-57815cc7c612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/otniel/miniconda3/envs/dl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time_total': 9.59500615700017,\n",
       " 'generated_tokens': 866,\n",
       " 'tokens/sec': 90.2552834078379}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_original, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab302b-d489-4e02-a05e-d4642a722246",
   "metadata": {},
   "source": [
    "---\n",
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074946b0-540d-494f-87e0-25024e6af5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " {'time_total': 96.33005910000065, 'generated_tokens': 8660, 'tokens/sec': 89.89924931957134}\n"
     ]
    }
   ],
   "source": [
    "res_1 = test(model_original, n_runs)\n",
    "print(\"Original:\\n\", res_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be091000-7ffc-426e-99b6-40006a6fd9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w_adapter = get_peft_model(model, peft_config)\n",
    "model_w_adapter = torch.compile(model_w_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a8ba7f0-ec28-42cb-a866-f79a2a84d28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Adapter:\n",
      " {'time_total': 149.31412317300055, 'generated_tokens': 8660, 'tokens/sec': 57.998532328828816}\n"
     ]
    }
   ],
   "source": [
    "res_2 = test(model_w_adapter, n_runs)\n",
    "print(\"With Adapter:\\n\", res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc3e3924-39b0-4741-a6b1-91394972dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_merged = model_w_adapter.merge_and_unload()\n",
    "model_merged = torch.compile(model_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a246638b-6c35-4727-8924-c15b2f29e6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged:\n",
      " {'time_total': 102.24086965600145, 'generated_tokens': 8660, 'tokens/sec': 84.70193993006265}\n"
     ]
    }
   ],
   "source": [
    "res_3 = test(model_merged, n_runs)\n",
    "print(\"Merged:\\n\", res_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e6a069-b31a-41e2-a922-8318faeef95a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
