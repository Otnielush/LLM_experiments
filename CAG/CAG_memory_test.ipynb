{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30de3e63-2291-410f-8ef7-1ffa7048c3aa",
   "metadata": {},
   "source": [
    "# Summary memory usage with CAG\n",
    "    LLama 8B in 4bit quantization just load model takes - 7Gb\n",
    "    With 24Gb GPU memory I can use cache with size of 32k tokens (in theory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0577cba3-73be-4b0e-aacf-a037e7ad99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from time import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb33d5-3a94-49ef-b03b-ca4f3a5d2567",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda719a9-c44d-4963-8fce-779d0b186ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/beauty-of-ares.txt\", 'r', encoding='utf-8') as f:\n",
    "    book = ''.join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a643008-a302-4f98-a4e4-b5d4154f72dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "Beauty of Ares\n",
      "Sleeping Beauty Inc. Book 3\n",
      "\n",
      "Stephanie Van Orman\n",
      "\n",
      "\f",
      "Copyright © 2023, 2024 Stephanie Van Orman\n",
      "All rights reserved. No part of this book may be reproduced or used in any manner without\n",
      "written permission of the copyright owner except for the use of written quotations in a book\n",
      "review.\n",
      "Any reference to historical events, real people or places are used fictitiously. Names, characters,\n",
      "places are products of the author’s imagination.\n",
      "Front cover image by Shutterstock\n",
      "Book design by S\n"
     ]
    }
   ],
   "source": [
    "print(book[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf54f38-2b0c-4033-b293-169208f1e44a",
   "metadata": {},
   "source": [
    "---\n",
    "# Generate cache\n",
    "from paper [repo](https://github.com/hhhuang/CAG/blob/main/kvcache.py#L79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c3dd442-ccf1-4edd-8e19-c77e0eb4bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_knowledge(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    ") -> DynamicCache:\n",
    "    \"\"\"\n",
    "    Prepare knowledge kv cache for CAG.\n",
    "    Args:\n",
    "        model: HuggingFace model with automatic device mapping\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        prompt: The knowledge to preprocess, which is basically a prompt\n",
    "\n",
    "    Returns:\n",
    "        DynamicCache: KV Cache\n",
    "    \"\"\"\n",
    "    embed_device = model.model.embed_tokens.weight.device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(embed_device)\n",
    "    past_key_values = DynamicCache()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "    return outputs.past_key_values\n",
    "\n",
    "\n",
    "def write_kv_cache(kv: DynamicCache, path: str):\n",
    "    \"\"\"\n",
    "    Write the KV Cache to a file.\n",
    "    \"\"\"\n",
    "    torch.save(kv, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b0f0128-be52-443d-858b-436bda3dc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_kvcache(documents, filepath: str = \"./data_cache/cache_knowledges.pt\", answer_instruction: str = None):\n",
    "    # Prepare the knowledges kvcache\n",
    "\n",
    "    if answer_instruction is None:\n",
    "        answer_instruction = \"Answer the question with detailed answer usin ONLY provided context.\"\n",
    "    knowledges = f\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are an assistant for giving short answers based on given context.<|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Context information is bellow.\n",
    "    ------------------------------------------------\n",
    "    {documents}\n",
    "    ------------------------------------------------\n",
    "    {answer_instruction}\n",
    "    Question:\n",
    "    \"\"\"\n",
    "    # Get the knowledge cache\n",
    "    t1 = time()\n",
    "    kv = preprocess_knowledge(model, tokenizer, knowledges)\n",
    "    print(\"kvlen: \", kv.key_cache[0].shape[-2])\n",
    "    if filepath:\n",
    "        write_kv_cache(kv, filepath)\n",
    "    t2 = time()\n",
    "    return kv, t2 - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58c4d7db-771e-48f6-982c-a86c6ab17272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,              # Load model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",      # Normalize float 4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute dtype for 4-bit base matrices\n",
    "    bnb_4bit_use_double_quant=True  # Use nested quantization\n",
    ")\n",
    "\n",
    "\n",
    "def load_quantized_model(model_name, hf_token=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"cuda\",         \n",
    "        trust_remote_code=True,     # Required for some models\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e8cd8f-b189-40e3-88b8-9bc676b1d7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a6bf37f84741659659ca4558022797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = load_quantized_model(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d086020-3faf-4cee-b6e0-c180823c0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vram():\n",
    "    free = torch.cuda.mem_get_info()[0] / 1024 ** 3\n",
    "    total = torch.cuda.mem_get_info()[1] / 1024 ** 3\n",
    "    total_cubes = 24\n",
    "    free_cubes = int(total_cubes * free / total)\n",
    "    print(f'VRAM: {total - free:.2f}/{total:.2f}GB\\t VRAM:[' + (\n",
    "            total_cubes - free_cubes) * '▮' + free_cubes * '▯' + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d117b41f-a8aa-4fce-a18d-3e994ad397bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM: 6.99/23.99GB\t VRAM:[▮▮▮▮▮▮▮▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯]\n"
     ]
    }
   ],
   "source": [
    "get_vram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5dbf98-23f9-43aa-99aa-2d3deb1912f0",
   "metadata": {},
   "source": [
    "---\n",
    "# Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018ecd1e-edfb-4e40-8057-626ec61012a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111016"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(book, return_tensors=\"pt\")\n",
    "input_ids.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "319c5530-8ec1-409e-b796-c73cc00f2401",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_crop = tokenizer.decode(input_ids[0, :32000])\n",
    "book_crop = '\\n'.join(book_crop.split('\\n')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d5ee5f-403b-4108-a425-48b8a64b0639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kvlen:  32061\n",
      "KVcache prepared in 6.443361282348633 seconds\n"
     ]
    }
   ],
   "source": [
    "knowledge_cache, prepare_time = prepare_kvcache(book_crop, filepath=None, answer_instruction=None)\n",
    "kv_len = knowledge_cache.key_cache[0].shape[-2]\n",
    "print(f\"KVcache prepared in {prepare_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ecd2c4f-4419-4cf3-a733-2e7b7d80f93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM: 23.99/23.99GB\t VRAM:[▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮]\n"
     ]
    }
   ],
   "source": [
    "get_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d790c60c-ac67-441a-ac62-cc75a0661c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.904296875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved(0) / 1024 ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320290e2-200b-4d50-b4cc-ceb5205e8c74",
   "metadata": {},
   "source": [
    "---\n",
    "# Ask some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9aaf7c7-cac7-4346-a759-4d938ad74191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(kv: DynamicCache, origin_len: int):\n",
    "    \"\"\"\n",
    "    Truncate the KV Cache to the original length.\n",
    "    \"\"\"\n",
    "    for i in range(len(kv.key_cache)):\n",
    "        kv.key_cache[i] = kv.key_cache[i][:, :, :origin_len, :]\n",
    "        kv.value_cache[i] = kv.value_cache[i][:, :, :origin_len, :]\n",
    "\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids: torch.Tensor,\n",
    "    past_key_values,\n",
    "    max_new_tokens: int = 300\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate text with greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        model: HuggingFace model with automatic device mapping\n",
    "        input_ids: Input token ids\n",
    "        past_key_values: KV Cache for knowledge\n",
    "        max_new_tokens: Maximum new tokens to generate\n",
    "    \"\"\"\n",
    "\n",
    "    embed_device = model.model.embed_tokens.weight.device\n",
    "\n",
    "    origin_ids = input_ids\n",
    "    input_ids = input_ids.to(embed_device)\n",
    "\n",
    "    output_ids = input_ids.clone()\n",
    "    next_token = input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(\n",
    "                input_ids=next_token, \n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = next_token_logits.argmax(dim=-1).unsqueeze(-1)\n",
    "            next_token = next_token.to(embed_device)\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            output_ids = torch.cat([output_ids, next_token], dim=1)\n",
    "\n",
    "            if next_token.item() in model.config.eos_token_id:\n",
    "                break\n",
    "    return output_ids[:, origin_ids.shape[-1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d08a2934-3524-4b46-b537-29c20a67f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(prompt, nocache=False, max_new_tokens=500):\n",
    "    t1 = time()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = generate(model, input_ids, DynamicCache() if nocache else knowledge_cache, max_new_tokens=max_new_tokens)\n",
    "    t2 = time() - t1\n",
    "    clean_up(knowledge_cache, kv_len)\n",
    "    print(f\"generate time: {t2:.2f} sec\\ntokens/sec: {len(output[0])/t2:.2f}\\n\\n\" + tokenizer.decode(output[0], skip_special_tokens=True, temperature=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4d1cf-48a1-4b07-a05e-9726cdf5cfd6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d751392-4706-4eda-9273-eda58916099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time: 0.45 sec\n",
      "tokens/sec: 2.22\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"What is the story in the book?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3532a3ca-f867-4c76-a40d-7a155410a1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time: 33.20 sec\n",
      "tokens/sec: 13.55\n",
      "\n",
      " of the story so far.\n",
      "\n",
      "The story so far is about Lisbet, a young woman who was sold to Vantz Bloomburg, a trillionaire on Mars, by her father through the Sleeping Beauty Inc. agency. Lisbet is struggling to adjust to her new life on Mars and her role as Vantz's wife. She is frustrated with her situation and feels like she is being treated like a possession rather than a person. She is also struggling to understand the true nature of her husband's business dealings and the circumstances surrounding her arrival on Mars.\n",
      "\n",
      "Lisbet is trying to navigate her new life on Mars, but she is finding it difficult to connect with her husband, Vantz, who is distant and uncommunicative. She is also struggling to understand the true nature of her role as a public relations officer for Vantz's terraforming project. She is frustrated with the lack of transparency and the secrecy surrounding the project.\n",
      "\n",
      "Lisbet is also struggling to connect with her new environment and the people around her. She is finding it difficult to make friends and is feeling isolated and alone. She is also struggling to understand the Martian culture and the customs of the people around her.\n",
      "\n",
      "Despite her struggles, Lisbet is determined to make the best of her situation and to find a way to make a positive impact on the world. She is also determined to uncover the truth about her husband's business dealings and the circumstances surrounding her arrival on Mars.\n",
      "\n",
      "The story is a commentary on the objectification of women and the commodification of the human body. It is also a commentary on the exploitation of the poor and the marginalized. The story highlights the power dynamics at play in the relationship between Lisbet and Vantz, and the ways in which Lisbet is being used as a tool for Vantz's business dealings.\n",
      "\n",
      "The story is also a commentary on the effects of loneliness and isolation on the human psyche. Lisbet is struggling to cope with the isolation and loneliness of her new life on Mars, and is finding it difficult to form meaningful connections with the people around her.\n",
      "\n",
      "Overall, the story is a complex and thought-provoking exploration of the human condition, and the ways in which societal expectations and power dynamics can shape our experiences and relationships.\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"Make summary from context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ebd421d-d082-4113-9b2c-709c1b63a492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time: 0.18 sec\n",
      "tokens/sec: 5.54\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"Who is the main 10 persons in book?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96ae14c7-8af5-43a9-a68a-d9db6db3e1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time: 0.18 sec\n",
      "tokens/sec: 5.67\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"Who is the Vantz Bloomburg?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3526d520-df10-451b-86a1-e6697f520d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time: 0.19 sec\n",
      "tokens/sec: 5.39\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"What is Sleeping Beauty?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aa710eb-a199-4c40-8683-17328ab169aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time: 0.19 sec\n",
      "tokens/sec: 5.15\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"question: Where lived Lisbet?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45949326-6240-41da-9033-974722d4100e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
